{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"house_price_prediction_dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"houses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SCOPE\n",
    "Here I try to get the requests with spark methods and sql queries. The requests according to `house_price_simple_analysis.py` file:\n",
    "01. Schema of the dataset.\n",
    "02. Row count of the dataset.\n",
    "03. Column count of the dataset.\n",
    "04. Describes of descriptive stats.\n",
    "05. Min of descriptive stats.\n",
    "06. Max of descriptive stats.\n",
    "07. Null control for some columns.\n",
    "08. Fill null rows.\n",
    "09. Bedrooms biggest than three.\n",
    "10. Year built < 2010.\n",
    "11. Location count.\n",
    "12. Price order by location.\n",
    "13. Price per square.\n",
    "14. Order by price.\n",
    "15. Price by area.\n",
    "16. Adding has_garage column to dataset.<br><br><br>\n",
    "***  \n",
    "##### TODO\n",
    "\n",
    "- [x] Schema of the dataset.\n",
    "- [x] Row count of the dataset.\n",
    "- [x] Column count of the dataset.\n",
    "- [ ] Describes of descriptive stats.\n",
    "- [ ] Min of descriptive stats.\n",
    "- [ ] Max of descriptive stats.\n",
    "- [ ] Null control for some columns.\n",
    "- [ ] Fill null rows.\n",
    "- [ ] Bedrooms biggest than three.\n",
    "- [ ] Year built < 2010.\n",
    "- [ ] Location count.\n",
    "- [ ] Price order by location.\n",
    "- [ ] Price per square.\n",
    "- [ ] Order by price.\n",
    "- [ ] Price by area.\n",
    "- [ ] Adding has_garage column to dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Area: integer (nullable = true)\n",
      " |-- Bedrooms: integer (nullable = true)\n",
      " |-- Bathrooms: integer (nullable = true)\n",
      " |-- Floors: integer (nullable = true)\n",
      " |-- YearBuilt: integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Condition: string (nullable = true)\n",
      " |-- Garage: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_of_dataset_py = df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|       Id|      int|   NULL|\n",
      "|     Area|      int|   NULL|\n",
      "| Bedrooms|      int|   NULL|\n",
      "|Bathrooms|      int|   NULL|\n",
      "|   Floors|      int|   NULL|\n",
      "|YearBuilt|      int|   NULL|\n",
      "| Location|   string|   NULL|\n",
      "|Condition|   string|   NULL|\n",
      "|   Garage|   string|   NULL|\n",
      "|    Price|      int|   NULL|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_of_dataset_sql = spark.sql(\"DESCRIBE houses;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count of the Dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "count_of_dataset_py = df.count()\n",
    "print(\"Row Count of the Dataset: \",count_of_dataset_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|     2000|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_of_dataset_sql = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) AS row_count FROM houses;\n",
    "    \"\"\"\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Count of the Dataset:  10\n"
     ]
    }
   ],
   "source": [
    "column_count_of_dataset_py = len(df.columns)\n",
    "print(\"Column Count of the Dataset: \", column_count_of_dataset_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|column_count|\n",
      "+------------+\n",
      "|          10|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_count_of_dataset_sql = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT SIZE(ARRAY(*)) AS column_count FROM houses LIMIT 1;\n",
    "    \"\"\"\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
