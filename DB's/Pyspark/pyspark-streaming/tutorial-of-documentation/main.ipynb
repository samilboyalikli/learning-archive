{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada Spark Streaming için bir DataFrame oluşturuyoruz. Bu DataFrame ile soket bağlantısından veri okuyacağız. <br><br>\n",
    "Sık kullanılan bazı metotlar:  \n",
    "`.readStream` - Bu komut, bir streaming kaynağından real-time veri okuma işlemini başlatır.  \n",
    "`.format()` - Bu metot, real-time verilerin hangi formatla geleceğini belirler.  \n",
    "`.option()` - Bu metot, DataFrame ayarlarında değişiklikler başlatır.  \n",
    "`.load()` - Bu metot, streaming kaynağını yükler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/21 18:14:35 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "lines = spark \\\n",
    "        .readStream \\\n",
    "        .format('socket') \\\n",
    "        .option(\"host\",\"localhost\") \\\n",
    "        .option(\"port\",9999) \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.format('socket')` - Burada veri formatını soket olarak belirledik.  \n",
    "`.option(\"host\", \"localhost\")` - Burada soketin host adresini localhost olarak belirledik.  \n",
    "`.option(\"port\", 9999)` - Burada soketin bağlanacağı port numarasını 9999 olarak belirledik.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada soketten alacağımız satırlardaki kelimeleri ayıklamak için bir DataFrame oluşturuyoruz.<br><br>\n",
    "\n",
    "> Burada önemli bir nokta var. `readStream` kullanılarak bir soketten veri okunduğunda Spark her bir satırı bir string olarak ele alır. Bu ele aldığı stringlerle default biçimde oluşturuduğu DataFrame ise tek sütunludur. Spark o sütuna `value` adını verir. Bunu nedenle `lines` olarak nitelediğimiz DataFrame aslında tek sütunlu bir tablodur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(\n",
    "    explode(\n",
    "        split(lines.value, \" \")\n",
    "    ).alias(\"word\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split(lines.value, \" \")` - `lines` DataFrame'inin `values` columnunu boşluklarından parçalıyoruz.  \n",
    "`alias(\"word\")` - Her bir parçaya `\"word\"` adını veriyoruz.  \n",
    "`explode()` - Tüm parçaları explode ediyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada okumuş olduğumuz streaminge (`readStream`) çeşitli işlemler (`words`+`wordCounts`) uyguladıktan sonra artık işlenmiş olan veriyi streaminge yazmaya (`writeStream`) başlıyoruz.  \n",
    "\n",
    "`wordCounts` - Yukarıdaki dümdüz batch-data analiz işlemleri.  \n",
    "`writeStream` - İşlenmiş veriyi stream olarak yazıyoruz.  \n",
    "`outputMode()` - Output modunu belirliyoruz.  \n",
    "`format()` - Outputun formatını belirliyoruz.  \n",
    "`start()` - Yazdığımız streami başlatıyoruz.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = wordCounts \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`outputMode(\"complete\")` - Çıktı modunu \"complete\" olarak ayarladık, böylece her mikro-batch işleminde tüm sonuçlar gösterilecek.  \n",
    "`format(\"console\")` - Çıktı formatını \"console\" olarak ayarladık, böylece output terminalde gösterilecek.<br><br>\n",
    "\n",
    "`awaitTermination()` - Streaming sorgusunu sürekli çalışır halde tutar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
